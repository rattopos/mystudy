# í†µê³„, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ - ë¥˜ê·¼ê´€

## 2025-07-03

IID: [Independent and identically distributed random variables - Wikipedia](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)
LLN: [Law of large numbers - Wikipedia](https://en.wikipedia.org/wiki/Law_of_large_numbers)

[Explainable artificial intelligence - Wikipedia](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)

[Shapley value - Wikipedia](https://en.wikipedia.org/wiki/Shapley_value)

ëª¨ë“ ê²Œ ì •ê·œë¶„í¬ë¡œ ê·¼ì‚¬ë˜ëŠ”ê²Œ ì•„ë‹ˆë‹¤.

í˜„ì‹¤ì˜ ì •ì¹˜ì  ê²¬í•´ê°€ ì •ê·œë¶„í¬ì™€ ë‹¤ë¥´ê²Œ ìŒë´‰ì¸ ì´ìœ 

ì¤‘ì‹¬ê·¹í•œ ì •ë¦¬ì˜ ê°€ì •: ê°ê°ì˜ ìš”ì¸ì´ 'ë…ë¦½'ì ìœ¼ë¡œ 'ì—‡ë¹„ìŠ·'í•˜ê²Œ ì˜í–¥ì„ ë¯¸ì³ì•¼ í•œë‹¤.

ì£¼ì‹ì‹œì¥ì˜ ìˆ˜ìµë¥ : ì •ê·œë¶„í¬ë³´ë‹¤ ê¼¬ë¦¬ê°€ ë‘ê»ë‹¤ (fat tail)

ì†Œë“ë¶„í¬: right skewed distribution, mean: 1ì¸ë‹¹ GDP

q1 q2 q3ë¥¼ ë³´ê³  skewedì¸ì§€ ì•½ê°„ íŒë‹¨ ê°€ëŠ¥

ë² ì´ì¦ˆ ë²•ì¹™

$$ P(A \mid B) = \frac{P(A)P(B\mid A)}{P(B)} $$

ë”°ë¼ì„œ

$$ \stackrel{\text{posterior}}{P(A \mid B)} \propto \stackrel{\text{prior}}{P(A)} \stackrel{\text{likelihood}}{P(B \mid A)} $$

likelihood, posterior, prior, marginal, joint

$$ P(A,B) = P(A) P(B \mid A) = P(B) P(A \mid B) $$

[Secretary problem - Wikipedia](https://en.wikipedia.org/wiki/Secretary_problem)

## 2025-07-07

${ \mu = ER }$ë¡œ ë‘ê³  í…Œì¼ëŸ¬ ì „ê°œë¥¼ í•˜ë©´,

$$ E \log (1+R) \approx \mu-\frac{1}{2}\frac{\sigma^{2}}{(1+\mu)^{2}} $$

ì£¼ì‹ì‹œì¥ì´ ë‰´ìŠ¤ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘ í•˜ëŠ” ì´ìœ  ì†ì ˆë§¤(stop loss)ë¥¼ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ìë™ìœ¼ë¡œ ê±¸ì–´ë‘ê¸° ë•Œë¬¸ì— ì—°ì‡„ë°˜ì‘ (fat tailì„ ì„¤ëª…)

[Bayesian updating](https://en.wikipedia.org/wiki/Bayesian_inference)

R passing stratege: ìµœê³ ì˜ ì „ëµì„ ë‹´ì€ class

H: ì •ë³´ ì—”íŠ¸ë¡œí”¼, MI: mutual information

$$ \operatorname{H}(X,Y) = \operatorname{H}(X) + \operatorname{H}(Y) - \mathrm{MI}(XY) $$

[Entropy (information theory) - Wikipedia](https://en.wikipedia.org/wiki/Entropy_\(information_theory\))
 
$$ \operatorname{H}(X) = \operatorname{E}_{X} \log_{2} \frac{1}{p(X)}  $$

${ \operatorname{MI}(X,Y) = }$ [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between ${ f(x,y) }$ and ${ f(x)f(y) }$

$$ p(X,Y) = \begin{dcases}
p(X) p(Y \mid X) \\
p(Y) p(X \mid Y)
\end{dcases} $$

$$ - \log_{2} p(X,Y) = \begin{dcases}
- \log_{2} p(X) - \log_{2} p(Y \mid X) \\
- \log_{2} p(Y) - \log_{2} p(X \mid Y)
\end{dcases}
$$

$$ \operatorname{H}(X,Y) = \operatorname{H}(X \mid Y) + \operatorname{H}(Y \mid X) + \operatorname{MI}(X,Y) $$

ì•„ë“¤ ì˜ ë‚³ëŠ” ì»¤í”Œ(B)ì´ ì•„ë“¤ ë‚³ì„ í™•ë¥  0.6

ë”¸ ì˜ ë‚³ëŠ” ì»¤í”Œì´(G) ì•„ë“¤ ë‚³ì„ í™•ë¥  0.4

ì²« ì•„ì´ê°€ ì•„ë“¤ì¼ ë•Œ ë‘˜ì§¸ë„ ì•„ë“¤ì¼ í™•ë¥ ?

B: ${ 0.6 \times 0.6 = 0.36 }$
G: ${ 0.4 \times 0.4 = 0.16 }$
Bì™€ Gê°€ ë…ë¦½ì´ë¯€ë¡œ ${ 0.52 }$
ë”°ë¼ì„œ ${ 0.6 }$ê³¼ 0.4ì˜ í‰ê· ì¸ ${ 0.5 }$ ë³´ë‹¤ í¬ë‹¤.

IID ë³´ë‹¤ [Exchangeablity](https://en.wikipedia.org/wiki/Exchangeable_random_variables)ê°€ í˜„ì‹¤ì„ ë” ì˜ ì„¤ëª…í•œë‹¤. (í™•ë¥ ë¶„í¬ê°€ ì¹˜ìš°ì³ì ¸ ìˆë‹¤)

ì½œì˜µì…˜

treasury bond: $1 â†’ (1+r)
stock: $1 â†’ 1+u, 1-u
call option (X = exercise price)
$c â†’ ${ (1+u) - X }$, ${ \max((1-u)-X,0)=0 }$

[Law of one price - Wikipedia](https://en.wikipedia.org/wiki/Law_of_one_price)

[Risk-neutral measure - Wikipedia](https://en.wikipedia.org/wiki/Risk-neutral_measure)

[ì œê³±ê·¼ì˜ ë²•ì¹™](https://www.britannica.com/science/probability-theory/The-central-limit-theorem#ref407420)
êµì¬ 250p.
$$ \text{(í•©ì˜ í‘œì¤€ì˜¤ì°¨)} = \text{(ìƒìì˜ í‘œì¤€í¸ì°¨)}\times \sqrt{\text{ì¶”ì¶œ íšŸìˆ˜}} $$
ì¶”ì¶œíšŸìˆ˜ê°€ 100ë°°ë©´ í‘œì¤€ì˜¤ì°¨ëŠ” 10ë°°
í†µê³„ê°€ ì—‰ë§ì¸ ì˜ˆ: "ë‚¨ìë¡œ í•œì •í•˜ë©´~" í‘œë³¸ì´ ì¤„ì–´ë“œë¯€ë¡œ ì˜¤ì°¨í•œê³„ê°€ ë‚®ì•„ì§„ë‹¤.

í•™êµì—ì„œëŠ” hypothesis testë§Œ ê°€ë¥´ì¹˜ëŠ”ë° í˜„ì‹¤ì€ model selection
cannot rejectëŠ” ì–´ì©”ê»€ë°? ì •ë§ ë¬´ì˜ë¯¸í•˜ë‹¤.(ìƒì‚°ì ì´ì§€ ì•ŠìŒ, ê²Œë¥´ë‹¤ëŠ” ê²ƒì„ ì‹œì¸í•˜ëŠ” ê²ƒ) ë¹…ë°ì´í„° ì‹œëŒ€ì— í‘œë³¸ì˜ ìˆ˜ê°€ ë¬´í•œëŒ€ë¡œ ê°€ë©´ ëª¨ë“  T-í…ŒìŠ¤íŠ¸ê°€ ë¦¬ì ëœë‹¤.

model í•˜ë‚˜ ì„¸ì›Œë‘ê³  í…ŒìŠ¤íŠ¸ í†µê³¼í–ˆë‹¤: nonsense... ê²Œìœ¼ë¥¸ ê²ƒ. ì œì¼ ì í•©í•œ modelì„ ê³¨ë¼ì•¼í•¨.

[All models are wrong - Wikipedia](https://en.wikipedia.org/wiki/All_models_are_wrong)

[Ensemble learning - Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)

accuracyì— ì§‘ì°©í•˜ëŠ” ê²ƒì€ ì˜ëª»ë˜ì—ˆë‹¤. ê·¸ëŸ°ê±´ í¬ê·€ë³‘ì—ë‚˜ ì˜ë¯¸ ìˆë‹¤.

ëŒ€ìˆ˜ì˜ ë²•ì¹™
1. big mouthê°€ ì—†ì–´ì•¼ í•œë‹¤
2. doubling strategyì—” ë¨¹íˆì§€ ì•ŠìŒ

í•œ ì¢…ëª©ì— ì˜¬ì¸í•œë‹¤ = ì™„ì „íˆ ê°™ì´ ì›€ì§ì´ëŠ” ë‘ ì¢…ëª©ì— 1/2, 1/2 ìƒê´€ê³„ìˆ˜ +1

ë‘ ì¢…ëª©ì¸ë° ìƒê´€ê³„ìˆ˜ê°€ 0.99ë©´ ë‹¤ë³€íš¨ê³¼ê°€ ìˆë‹¤?

R1, R2 indep.
${ \text{Corr}(R_{1},R_{2}) = \rho }$

$$ \begin{eqnarray} Var(R) & = & E(R - ER)^{2} \\
& = & E(0.5(R_{1}-ER_{1}) + 0.5(R_{2}-ER_{2})) \\ 
& = & \frac{1}{4} \sigma_{1}^{2} + \frac{1}{4}\sigma_{2}^{2} + \frac{1}{2}\rho \sigma_{1} \sigma_{2} \\
& \stackrel{\sigma_{1}^{2}=\sigma_{2}^{2}}{=} & \frac{\sigma^{2}}{2} (1+\rho) \stackrel{-1 \le \rho \le 1}{\le} \sigma^{2}
\end{eqnarray} $$

$$ \sigma_{R} = \sqrt{\frac{1+\rho}{2}}\sigma $$

special cases
1. ${ \rho = +1 \implies \sigma_{R} = \sigma }$
2. ${ \rho = -1 \implies \sigma_{R}=0 }$
3. ${ \rho = 0 \implies \sigma_{R} = \frac{\sigma}{\sqrt{2} } }$
In general (${ \rho \neq \pm 1 ) }$, ${ \sigma_{R}<\sigma }$

[Law of total expectation - Wikipedia](https://en.wikipedia.org/wiki/Law_of_total_expectation)

[Law of total variance - Wikipedia](https://en.wikipedia.org/wiki/Law_of_total_variance)

## 2025-07-10

ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°: [binomial model of stock prices](https://en.wikipedia.org/wiki/Risk-neutral_measure#Example_1_%E2%80%93_Binomial_model_of_stock_prices); [Blackâ€“Scholes model - Wikipedia](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model)ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒë„ ì•Œ ìˆ˜ ìˆìŒ.

ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ë²•ì— í—·ê°ˆë¦¬ëŠ”ê²Œ ìˆëŠ”ë° ê·¸ê±° ë°˜ë“œì‹œ ìƒê°í•´ë³´ê¸°

ì–´ë–¤ ë¶„í¬ë¥¼ ì •ê·œë¶„í¬ë¡œ ê°€ì •í•˜ëŠ” ê°€ì •í•  í•„ìš”ê°€ ì—†ì–´ì¡Œë‹¤. ì»´í“¨í„°ê°€ ë§ì´ ë°œë‹¬í–ˆê¸° ë•Œë¬¸ì— sampling with replacementë¡œ í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ì´ ë” ì •í™•í•˜ë‹¤.

KL - divergence

$$ KL(p \parallel q) = E_{p}(\log p - \log q) = \stackrel{\text{cross entropy}}{(-E_{p} \log q)} - (\stackrel{\text{own entropy}}{-E_{p} \log p})$$
|              | ì¢‹ì€ ì½”ë”©   | ë‚˜ìœ ì½”ë”©         |
| ------------ | ------- | ------------- |
| sunny, 0.5   | 1, 0.5  | 01, 1, 0.25   |
| rainy, 0.25  | 01, 0.5 | 1, 0.25, 0.5  |
| cloudy, 0.25 | 00, 0.5 | 00, 0.5, 0.25 |
| í‰ê·  ë¹„íŠ¸        | 1.5     | 1.75          |
ê±°ê¾¸ë¡œ ${ \log_{2} \frac{1}{q}  }$ë¡œ ì›ë˜ í™•ë¥ ë¶„í¬ë¥¼ ìœ ì¶”í•  ìˆ˜ ìˆë‹¤.

$$ -KL(p \parallel q) = E_{p}(\log \frac{q}{p}) \stackrel{\text{Jensen}}{\le} \log(E_{p}\frac{q}{p}) $$

ë“±í˜¸ëŠ” q/pê°€ ìƒìˆ˜ì¼ë•Œ, ì¦‰ q/p=1

Deep Learning: KL-divergenceì˜ ê°’ì„ minimize í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ training (ì›ë˜ ë¶„í¬ì™€ ê°€ê¹ê²Œ?)
${ p }$ë¥¼ ${ \hat{p} }$ìœ¼ë¡œ ê·¼ì‚¬ (empirical distribution)

ìµœí›„ì¶”ì •ë²•(ì‚¬ì§„ ì°¸ê³ ) MLE

distanceê°€ ì•„ë‹ˆë¼ divergenceë¼ê³  ë¶€ë¥´ëŠ” ì´ìœ 
pì™€ qì— ëŒ€ì¹­ì„±ì´ ì—†ì–´ì„œ.
ìƒí™©ì— ë”°ë¼ì„œ që¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ëŠ” ê²½ìš°ë„ ìˆìŒ ë” í¸ë¦¬í•œ ìª½ì„ ì„ íƒ.

ìœ„ ì½”ë“œì˜ˆì‹œì—ì„œ KL(p||q) ì™€ KL(q||p) ê³„ì‚°í•´ë³´ê¸° (ë˜‘ê°™ì´ ë‚˜ì˜´)

ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°: ë‹¤ë¥´ê²Œ ë‚˜ì˜¤ëŠ” ì˜ˆì‹œ ë§Œë“¤ì–´ë³´ê¸°

review

| name           | inequality                                                            |
| -------------- | --------------------------------------------------------------------- |
| Markov         | ${ P(Y \ge c) \le \frac{E(Y)}{c} }$                                   |
| Chebyshev      | ${ P(\left\lvert Y-EY \right\rvert \ge k\sigma) \le \frac{1}{k^{2}}}$ |
| Cauchy-Schwarz | ${ E(X^{2}) E(Y^{2}) \ge E(X)^{2}E(Y)^{2} }$                          |

SKì¦ê¶Œ JPëª¨ê±´
ì„ ìˆ˜ë¼ë¦¬ ê±°ë˜í•˜ëŠ”ë° ìœ„í—˜ì€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•´ì•¼ì§€...

ì•ìœ¼ë¡œ 1ë…„ê³¼ ê³¼ê±° 1ë…„ì´ ê°™ë‹¤ê³  ëª¨ë¸ì„ ì„¸ìš´ë‹¤. (ê°™ì„ë¦¬ê°€ ì—†ì§€ë§Œ ì‹¤ìš©ì ì¸ ì´ìœ ë¡œ)

financial bootstraping

ê°•ë‚¨ì•„íŒŒíŠ¸ ê°€ê²© ì„ í˜•íšŒê·€ ì‹¤ì œë¡œ ê³„ì‚°í•´ë³´ê¸°

[Omitted-variable bias - Wikipedia](https://en.wikipedia.org/wiki/Omitted-variable_bias): í˜„ì‹¤ì—ì„œ ë„ˆë¬´ ë„ˆë¬´ ë§ë‹¤

[Bias (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Bias_(statistics))

aggregation bias

regression effect

regressionì˜ ìœ ë˜: ê°ˆíŠ¼ì˜ í‚¤ ì—°êµ¬

$$ \left\lvert b \right\rvert \le \frac{SD_{Y}}{SD_{X}} $$

ìŠ¤í¬ì¸  ê¸°ìë“¤ì´ ë£¨í‚¤ë³´ê³  ìŠ¬ëŸ¼í”„ë¼ê³  ì˜¤íŒ

ê²½ì œí•™ìë“¤ì´ GDP íšŒê·€ë¶„ì„ í•˜ëŠ”ë° ìˆ˜ë ´í• ê±°ë¼ê³  ì˜¤íŒ

íšŒê·€ë¶„ì„ì˜ ê¸°ìš¸ê¸°ê°€ ì‹œê°„ì´ ê°ˆ ìˆ˜ë¡ 0ìœ¼ë¡œ ê°€ëŠ” ì´ìœ ì˜ ì˜¬ë°”ë¥¸ í•´ì„ì€ Yì˜ ì°¨ì´ë¥¼ ë” ì´ìƒ Xì˜ ì°¨ì´ë¡œ ì„¤ëª…í• ê²Œ ì—†ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. Yì˜ ì°¨ì´ê°€ ì—†ë‹¤ê°€ ì•„ë‹˜.

ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°
- [ ] binomial of stock price
- [ ] KL-divergence ë¹„ëŒ€ì¹­ì¸ê±° ì°¾ì•„ë³´ê¸°
- [ ] í™•ë¥ ë¡  ë¶€ë“±ì‹ ì¦ëª…
- [ ] ë¶€íŠ¸ìŠ¤íŠ¸ë©í•‘ í—·ê°ˆë¦¬ëŠ”ê±° ì˜ ìƒê°í•´ë³´ê¸°
- [ ] financial bootstraping ì°¾ì•„ë³´ê¸°
- [ ] ì „ë¯¸ë¶„ì´ ì™œ ì „ë¯¸ë¶„ì¸ì§€ ìƒê°í•´ë³´ê¸°
- [ ] ê°•ë‚¨ì•„íŒŒíŠ¸ ê°€ê²© ë°ì´í„° ë°›ì•„ì„œ íŒŒì´ì¬ìœ¼ë¡œ ì„ í˜•íšŒê·€ ì‹¤ì œë¡œ ê³„ì‚°í•´ë³´ê¸°
- [ ] ê°•ë‚¨ì•„íŒŒíŠ¸ ê°€ê²©ì„ í†µí•´ì„œ ì™œ ë³€ìˆ˜ í†µì œê°€ ì¤‘ìš”í•œì§€ ìƒê°í•´ë³´ê¸° (omitted-variable bias) í”„ë¡œì ì…˜ ì´ë¦¬ì €ë¦¬ í•´ë³´ê¸°.
- [ ] ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²• ì˜¤ì°¨ì˜ í•œê³„ ìƒê°í•´ë³´ê¸° (ì˜¤ëŠ˜ë°°ìš´ ë‚´ìš© (ì•„ë§ˆ ì œê³±ê·¼ì˜ ë²•ì¹™?)ê³¼ ì—°ê´€í•´ì„œ ìƒê°í•´ë³´ê¸°

## 2025-07-14

[Confounding - Wikipedia](https://en.wikipedia.org/wiki/Confounding)

accuracy maximize ë³´ë‹¨ objective oriented inference

### ğŸ“ ë¥˜ê·¼ê´€ êµìˆ˜ë‹˜ì˜ ë§¥ë½ì—ì„œ "Objective-Oriented Inference"ë€?

ë‹¤ìŒê³¼ ê°™ì€ ì•„ì´ë””ì–´ì— ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤:

#### âœ… **ëª©ì  ì§€í–¥ ì¶”ë¡  vs. ì§„ë¦¬ ì§€í–¥ ì¶”ë¡ **

- **ì§„ë¦¬ ì§€í–¥ì  ì¶”ë¡  (Truth-oriented inference)**:  
    â†’ "ëª¨ìˆ˜(parameter)ì˜ ì§„ì§œ ê°’ì„ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì¶”ì •í•˜ëŠëƒ"ì— ì´ˆì   
    â†’ ì „í†µì ì¸ í†µê³„í•™(ì˜ˆ: Neyman-Pearson, classical estimation)
    
- **ëª©ì  ì§€í–¥ì  ì¶”ë¡  (Objective-oriented inference)**:  
    â†’ "ê·¸ ì¶”ì •ì´ ì‹¤ì œ ì˜ì‚¬ê²°ì •ì´ë‚˜ ì •ì±… ëª©í‘œì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠëƒ"ì— ì´ˆì   
    â†’ ì¶”ì •ì˜ **ì •í™•ì„±**ë³´ë‹¤ **ìœ ìš©ì„±/íš¨ìš©ì„±**ì´ ì¤‘ìš”  
    â†’ ì˜ˆ: ì˜ëª»ëœ ì¶”ì •ì´ë”ë¼ë„, ì •ì±…ê²°ì •ì— ë„ì›€ì´ ëœë‹¤ë©´ ê·¸ê²Œ ë” ì¤‘ìš”í•œ ê²ƒ
    

---

### ğŸ“Œ ì˜ˆì‹œ (ê²½ì œí•™Â·ì •ì±… ë¶„ì„ ê´€ì ì—ì„œ)

- ì–´ë–¤ êµìœ¡ì •ì±…ì´ í•™ìƒì˜ ì„±ì ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë¶„ì„í•  ë•Œ:
    
    - **ì§„ë¦¬ ì§€í–¥ì  ì¶”ë¡ **: ì •ì±…ì˜ íš¨ê³¼ë¥¼ ì—„ë°€íˆ ì¶”ì •í•˜ëŠ” ë° ì´ˆì .
        
    - **ëª©ì  ì§€í–¥ì  ì¶”ë¡ **: ì •ì±…ì´ ì‹¤ì œë¡œ ì„±ê³¼ë¥¼ ê°œì„ í•˜ê²Œ ìœ ë„í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ê²ƒì— ì´ˆì .
        
- ì˜ë£Œë¹„ ë°ì´í„°ë¥¼ ë¶„ì„í•  ë•Œ:
    
    - ì§„ë¦¬ ì§€í–¥: í‰ê·  ì˜ë£Œë¹„ ì •í™•íˆ ì¶”ì •.
        
    - ëª©ì  ì§€í–¥: ê·¸ ì¶”ì •ê°’ì´ ë³´í—˜ë£Œ ì±…ì •ì´ë‚˜ ì •ì±… ì„¤ê³„ì— ì˜ ì‘ë™í•˜ëŠëƒê°€ ë” ì¤‘ìš”.
        

---

### ğŸ“˜ ë¥˜ êµìˆ˜ë‹˜ì˜ êµì¬ë‚˜ ê°•ì˜ ìŠ¬ë¼ì´ë“œì—ì„  ì£¼ë¡œ ë‹¤ìŒì²˜ëŸ¼ ìš”ì•½ë©ë‹ˆë‹¤:

> â€œìš°ë¦¬ëŠ” ë‹¨ì§€ parameter ê°’ì„ ì •í™•íˆ ì¶”ì •í•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼,  
> ê·¸ ì¶”ì •ì´ ì˜ì‚¬ê²°ì •ì— ì–´ë–¤ ë„ì›€ì„ ì£¼ëŠ”ì§€ì— ê´€ì‹¬ì´ ìˆë‹¤.â€

ì´ ê´€ì ì€ **í†µê³„í•™ì´ ì‹¤ì²œì  ì˜ì‚¬ê²°ì • ë„êµ¬ë¡œì„œ ê¸°ëŠ¥í•´ì•¼ í•œë‹¤**ëŠ” ì‹¤ìš©ì£¼ì˜ì  ì² í•™ì— ê¸°ë°˜í•œ ê²ƒì´ê³ , íŠ¹íˆ **ì •ì±…í•™, ê³„ëŸ‰ê²½ì œí•™, ì‚¬íšŒê³¼í•™** ìª½ì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ê²Œ ë‹¤ë¤„ì§‘ë‹ˆë‹¤.

---

### ğŸ” ìš”ì•½

|êµ¬ë¶„|ì„¤ëª…|
|---|---|
|**Truth-oriented**|parameter ê°’ì„ ì •í™•íˆ ì¶”ì •í•˜ë ¤ëŠ” ì „í†µì  í†µê³„ ì ‘ê·¼|
|**Objective-oriented**|ì‹¤ìš©ì Â·ì •ì±…ì  ëª©ì ì— ë§ëŠ” ìœ ìš©í•œ ì¶”ë¡ ì„ ê°•ì¡°|

[Hedonic regression - Wikipedia](https://en.wikipedia.org/wiki/Hedonic_regression)

ë°ì´í„°ë¥¼ ë¬´ì‘ì • ë§ì´ ë„£ëŠ”ë‹¤ê³  ì¢‹ì€ê²Œ ì•„ë‹ˆë‹¤, í†µì œëœ ë°ì´í„°ë¥¼ ë„£ì§€ ì•Šìœ¼ë©´ í†µê³„ë¥¼ ì™œê³¡í•˜ê²Œ ëœë‹¤.

[ë…¸ë²¨ìƒì´ ë°íŒ ë‚¨ë…€ ì„ê¸ˆê²©ì°¨ì˜ ì§„ì‹¤](https://brunch.co.kr/@brazilclub/132)

linear spline regressionë„ ì¤‘íšŒê·€ ë¶„ì„:
ê³„ìˆ˜ ${ \beta_{1},\beta_{2} }$ ë‘ ê°œë¡œ 2ì°¨ì› í‰ë©´ì„ ë§Œë“¤ê¸° ë•Œë¬¸ì¸ê°€?

ë„ë©”ì¸ ì§€ì‹ì´ ì—†ëŠ” ìƒíƒœë¡œ ë°ì´í„° ë¶„ì„ì„ í•˜ëŠ” ê²ƒë§Œí¼ ìœ„í—˜í•œê²Œ ì—†ë‹¤.

ì¡°ì •ëœ ê²°ì •ê³„ìˆ˜: ììœ ë„ë¡œ ë‚˜ëˆ ì¤˜ì„œ ì„¤ëª…ë³€ìˆ˜ê°€ ì •ë§ë¡œ ì œëŒ€ë¡œ ì—­í• ì„ í•˜ëŠ”ì§€ íŒë‹¨í•  ìˆ˜ ìˆë‹¤. <- ì§€ê¸ˆì€ ì˜ ì•ˆ ì”€

KNN: í•œ ì‚¬ëŒì„ ì•Œë ¤ë©´, ê·¸ ì‚¬ëŒì˜ ì¹œêµ¬ë¥¼ ë³´ë¼.
birds of a feather flock together, ìœ ìœ ìƒì¢…

${ y = f(x) + \varepsilon }$

$$ y - \hat{f}(x) = \overbrace{y - E(y \mid x)}^{A} + \overbrace{E(y\mid x) - f(x)}^{B} + \overbrace{f(x) - \hat{f}(x)}^{C} $$

Want ${ E[y\mid x] }$

- A: ì—ë¼ ëª¨ë¥´ê² ë‹¤
- B: estimation error
- C: sampling error


$$ \text{MSE} := E\left(\left( f(x) - \hat{f}(x)\right) \right) = \sigma^{2} + \mathrm{Bias}^{2} + \mathrm{Var}^{2} $$

- ${ \sigma^{2} = E(A^{2}) }$
- ${ \mathrm{Bias}^{2} = E(B^{2}) }$
- ${ \mathrm{Var}^{2} = E(C^{2}) }$

- [ ] A,B,C ìŒë§ˆë‹¤ correlationì´ 0ì¸ê±° ìƒê°í•´ë³´ê¸°

íŒ¨ë„í‹°ë¥¼ ì£¼ëŠ” ì„¸ê¸°ì— ë”°ë¼ ëª¨ë¸ì´ ê²°ì •ëœë‹¤. 

[Hyperparameter (Bayesian statistics) - Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_(Bayesian_statistics))

[Additive smoothing - Wikipedia](https://en.wikipedia.org/wiki/Additive_smoothing)

[Perronâ€“Frobenius theorem - Wikipedia](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem)

[Shrinkage (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Shrinkage_(statistics))

---

## ì œ1ì¥ í†µê³„í•™ê³¼ ìë£Œ

ëª¨ì§‘ë‹¨(population): ê´€ì‹¬ì´ ë˜ëŠ” ì§‘ë‹¨ ì „ì²´
í‘œë³¸(sample): ì¡°ì‚¬ëœ ì¼ë¶€
ëª¨ìˆ˜(parameter): ëª¨ì§‘ë‹¨ì˜ íŠ¹ì„±
í†µê³„ëŸ‰(statistic): í‘œë³¸ì˜ íŠ¹ì„±
ì¶”ë¡ (inference): í‘œë¶„ì„ ë¶„ì„í•˜ì—¬ ëª¨ì§‘ë‹¨ì˜ íŠ¹ì„±ì„ ì§ì‘

## ì œ10ì¥ ì´í•­ê³µì‹