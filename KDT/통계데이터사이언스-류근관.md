# í†µê³„, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ - ë¥˜ê·¼ê´€

## 2025-07-03

IID: [Independent and identically distributed random variables - Wikipedia](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)
LLN: [Law of large numbers - Wikipedia](https://en.wikipedia.org/wiki/Law_of_large_numbers)

[Explainable artificial intelligence - Wikipedia](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)

[Shapley value - Wikipedia](https://en.wikipedia.org/wiki/Shapley_value)

ëª¨ë“ ê²Œ ì •ê·œë¶„í¬ë¡œ ê·¼ì‚¬ë˜ëŠ”ê²Œ ì•„ë‹ˆë‹¤.

í˜„ì‹¤ì˜ ì •ì¹˜ì  ê²¬í•´ê°€ ì •ê·œë¶„í¬ì™€ ë‹¤ë¥´ê²Œ ìŒë´‰ì¸ ì´ìœ 

ì¤‘ì‹¬ê·¹í•œ ì •ë¦¬ì˜ ê°€ì •: ê°ê°ì˜ ìš”ì¸ì´ 'ë…ë¦½'ì ìœ¼ë¡œ 'ì—‡ë¹„ìŠ·'í•˜ê²Œ ì˜í–¥ì„ ë¯¸ì³ì•¼ í•œë‹¤.

ì£¼ì‹ì‹œì¥ì˜ ìˆ˜ìµë¥ : ì •ê·œë¶„í¬ë³´ë‹¤ ê¼¬ë¦¬ê°€ ë‘ê»ë‹¤ (fat tail)

ì†Œë“ë¶„í¬: right skewed distribution, mean: 1ì¸ë‹¹ GDP

q1 q2 q3ë¥¼ ë³´ê³  skewedì¸ì§€ ì•½ê°„ íŒë‹¨ ê°€ëŠ¥

ë² ì´ì¦ˆ ë²•ì¹™

$$ P(A \mid B) = \frac{P(A)P(B\mid A)}{P(B)} $$

ë”°ë¼ì„œ

$$ \stackrel{\text{posterior}}{P(A \mid B)} \propto \stackrel{\text{prior}}{P(A)} \stackrel{\text{likelihood}}{P(B \mid A)} $$

likelihood, posterior, prior, marginal, joint

$$ P(A,B) = P(A) P(B \mid A) = P(B) P(A \mid B) $$

[Secretary problem - Wikipedia](https://en.wikipedia.org/wiki/Secretary_problem)

## 2025-07-07

${ \mu = ER }$ë¡œ ë‘ê³  í…Œì¼ëŸ¬ ì „ê°œë¥¼ í•˜ë©´,

$$ E \log (1+R) \approx \mu-\frac{1}{2}\frac{\sigma^{2}}{(1+\mu)^{2}} $$

ì£¼ì‹ì‹œì¥ì´ ë‰´ìŠ¤ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘ í•˜ëŠ” ì´ìœ  ì†ì ˆë§¤(stop loss)ë¥¼ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ìë™ìœ¼ë¡œ ê±¸ì–´ë‘ê¸° ë•Œë¬¸ì— ì—°ì‡„ë°˜ì‘ (fat tailì„ ì„¤ëª…)

[Bayesian updating](https://en.wikipedia.org/wiki/Bayesian_inference)

R passing stratege: ìµœê³ ì˜ ì „ëµì„ ë‹´ì€ class

H: ì •ë³´ ì—”íŠ¸ë¡œí”¼, MI: mutual information

$$ \operatorname{H}(X,Y) = \operatorname{H}(X) + \operatorname{H}(Y) - \mathrm{MI}(XY) $$

[Entropy (information theory) - Wikipedia](https://en.wikipedia.org/wiki/Entropy_\(information_theory\))
 
$$ \operatorname{H}(X) = \operatorname{E}_{X} \log_{2} \frac{1}{p(X)}  $$

${ \operatorname{MI}(X,Y) = }$ [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between ${ f(x,y) }$ and ${ f(x)f(y) }$

$$ p(X,Y) = \begin{dcases}
p(X) p(Y \mid X) \\
p(Y) p(X \mid Y)
\end{dcases} $$

$$ - \log_{2} p(X,Y) = \begin{dcases}
- \log_{2} p(X) - \log_{2} p(Y \mid X) \\
- \log_{2} p(Y) - \log_{2} p(X \mid Y)
\end{dcases}
$$

$$ \operatorname{H}(X,Y) = \operatorname{H}(X \mid Y) + \operatorname{H}(Y \mid X) + \operatorname{MI}(X,Y) $$

ì•„ë“¤ ì˜ ë‚³ëŠ” ì»¤í”Œ(B)ì´ ì•„ë“¤ ë‚³ì„ í™•ë¥  0.6

ë”¸ ì˜ ë‚³ëŠ” ì»¤í”Œì´(G) ì•„ë“¤ ë‚³ì„ í™•ë¥  0.4

ì²« ì•„ì´ê°€ ì•„ë“¤ì¼ ë•Œ ë‘˜ì§¸ë„ ì•„ë“¤ì¼ í™•ë¥ ?

B: ${ 0.6 \times 0.6 = 0.36 }$
G: ${ 0.4 \times 0.4 = 0.16 }$
Bì™€ Gê°€ ë…ë¦½ì´ë¯€ë¡œ ${ 0.52 }$
ë”°ë¼ì„œ ${ 0.6 }$ê³¼ 0.4ì˜ í‰ê· ì¸ ${ 0.5 }$ ë³´ë‹¤ í¬ë‹¤.

IID ë³´ë‹¤ [Exchangeablity](https://en.wikipedia.org/wiki/Exchangeable_random_variables)ê°€ í˜„ì‹¤ì„ ë” ì˜ ì„¤ëª…í•œë‹¤. (í™•ë¥ ë¶„í¬ê°€ ì¹˜ìš°ì³ì ¸ ìˆë‹¤)

ì½œì˜µì…˜

treasury bond: $1 â†’ (1+r)
stock: $1 â†’ 1+u, 1-u
call option (X = exercise price)
$c â†’ ${ (1+u) - X }$, ${ \max((1-u)-X,0)=0 }$

[Law of one price - Wikipedia](https://en.wikipedia.org/wiki/Law_of_one_price)

[Risk-neutral measure - Wikipedia](https://en.wikipedia.org/wiki/Risk-neutral_measure)

[ì œê³±ê·¼ì˜ ë²•ì¹™](https://www.britannica.com/science/probability-theory/The-central-limit-theorem#ref407420)
êµì¬ 250p.
$$ \text{(í•©ì˜ í‘œì¤€ì˜¤ì°¨)} = \text{(ìƒìì˜ í‘œì¤€í¸ì°¨)}\times \sqrt{\text{ì¶”ì¶œ íšŸìˆ˜}} $$
ì¶”ì¶œíšŸìˆ˜ê°€ 100ë°°ë©´ í‘œì¤€ì˜¤ì°¨ëŠ” 10ë°°
í†µê³„ê°€ ì—‰ë§ì¸ ì˜ˆ: "ë‚¨ìë¡œ í•œì •í•˜ë©´~" í‘œë³¸ì´ ì¤„ì–´ë“œë¯€ë¡œ ì˜¤ì°¨í•œê³„ê°€ ë‚®ì•„ì§„ë‹¤.

í•™êµì—ì„œëŠ” hypothesis testë§Œ ê°€ë¥´ì¹˜ëŠ”ë° í˜„ì‹¤ì€ model selection
cannot rejectëŠ” ì–´ì©”ê»€ë°? ì •ë§ ë¬´ì˜ë¯¸í•˜ë‹¤.(ìƒì‚°ì ì´ì§€ ì•ŠìŒ, ê²Œë¥´ë‹¤ëŠ” ê²ƒì„ ì‹œì¸í•˜ëŠ” ê²ƒ) ë¹…ë°ì´í„° ì‹œëŒ€ì— í‘œë³¸ì˜ ìˆ˜ê°€ ë¬´í•œëŒ€ë¡œ ê°€ë©´ ëª¨ë“  T-í…ŒìŠ¤íŠ¸ê°€ ë¦¬ì ëœë‹¤.

model í•˜ë‚˜ ì„¸ì›Œë‘ê³  í…ŒìŠ¤íŠ¸ í†µê³¼í–ˆë‹¤: nonsense... ê²Œìœ¼ë¥¸ ê²ƒ. ì œì¼ ì í•©í•œ modelì„ ê³¨ë¼ì•¼í•¨.

[All models are wrong - Wikipedia](https://en.wikipedia.org/wiki/All_models_are_wrong)

[Ensemble learning - Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)

accuracyì— ì§‘ì°©í•˜ëŠ” ê²ƒì€ ì˜ëª»ë˜ì—ˆë‹¤. ê·¸ëŸ°ê±´ í¬ê·€ë³‘ì—ë‚˜ ì˜ë¯¸ ìˆë‹¤.

ëŒ€ìˆ˜ì˜ ë²•ì¹™
1. big mouthê°€ ì—†ì–´ì•¼ í•œë‹¤
2. doubling strategyì—” ë¨¹íˆì§€ ì•ŠìŒ

í•œ ì¢…ëª©ì— ì˜¬ì¸í•œë‹¤ = ì™„ì „íˆ ê°™ì´ ì›€ì§ì´ëŠ” ë‘ ì¢…ëª©ì— 1/2, 1/2 ìƒê´€ê³„ìˆ˜ +1

ë‘ ì¢…ëª©ì¸ë° ìƒê´€ê³„ìˆ˜ê°€ 0.99ë©´ ë‹¤ë³€íš¨ê³¼ê°€ ìˆë‹¤?

R1, R2 indep.
${ \text{Corr}(R_{1},R_{2}) = \rho }$

$$ \begin{eqnarray} Var(R) & = & E(R - ER)^{2} \\
& = & E(0.5(R_{1}-ER_{1}) + 0.5(R_{2}-ER_{2})) \\ 
& = & \frac{1}{4} \sigma_{1}^{2} + \frac{1}{4}\sigma_{2}^{2} + \frac{1}{2}\rho \sigma_{1} \sigma_{2} \\
& \stackrel{\sigma_{1}^{2}=\sigma_{2}^{2}}{=} & \frac{\sigma^{2}}{2} (1+\rho) \stackrel{-1 \le \rho \le 1}{\le} \sigma^{2}
\end{eqnarray} $$

$$ \sigma_{R} = \sqrt{\frac{1+\rho}{2}}\sigma $$

special cases
1. ${ \rho = +1 \implies \sigma_{R} = \sigma }$
2. ${ \rho = -1 \implies \sigma_{R}=0 }$
3. ${ \rho = 0 \implies \sigma_{R} = \frac{\sigma}{\sqrt{2} } }$
In general (${ \rho \neq \pm 1 ) }$, ${ \sigma_{R}<\sigma }$

[Law of total expectation - Wikipedia](https://en.wikipedia.org/wiki/Law_of_total_expectation)

[Law of total variance - Wikipedia](https://en.wikipedia.org/wiki/Law_of_total_variance)

## 2025-07-10

ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°: [binomial model of stock prices](https://en.wikipedia.org/wiki/Risk-neutral_measure#Example_1_%E2%80%93_Binomial_model_of_stock_prices); [Blackâ€“Scholes model - Wikipedia](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model)ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒë„ ì•Œ ìˆ˜ ìˆìŒ.

ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ë²•ì— í—·ê°ˆë¦¬ëŠ”ê²Œ ìˆëŠ”ë° ê·¸ê±° ë°˜ë“œì‹œ ìƒê°í•´ë³´ê¸°

ì–´ë–¤ ë¶„í¬ë¥¼ ì •ê·œë¶„í¬ë¡œ ê°€ì •í•˜ëŠ” ê°€ì •í•  í•„ìš”ê°€ ì—†ì–´ì¡Œë‹¤. ì»´í“¨í„°ê°€ ë§ì´ ë°œë‹¬í–ˆê¸° ë•Œë¬¸ì— sampling with replacementë¡œ í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ì´ ë” ì •í™•í•˜ë‹¤.

KL - divergence

$$ KL(p \parallel q) = E_{p}(\log p - \log q) = \stackrel{\text{cross entropy}}{(-E_{p} \log q)} - (\stackrel{\text{own entropy}}{-E_{p} \log p})$$
|              | ì¢‹ì€ ì½”ë”©   | ë‚˜ìœ ì½”ë”©         |
| ------------ | ------- | ------------- |
| sunny, 0.5   | 1, 0.5  | 01, 1, 0.25   |
| rainy, 0.25  | 01, 0.5 | 1, 0.25, 0.5  |
| cloudy, 0.25 | 00, 0.5 | 00, 0.5, 0.25 |
| í‰ê·  ë¹„íŠ¸        | 1.5     | 1.75          |
ê±°ê¾¸ë¡œ ${ \log_{2} \frac{1}{q}  }$ë¡œ ì›ë˜ í™•ë¥ ë¶„í¬ë¥¼ ìœ ì¶”í•  ìˆ˜ ìˆë‹¤.

$$ -KL(p \parallel q) = E_{p}(\log \frac{q}{p}) \stackrel{\text{Jensen}}{\le} \log(E_{p}\frac{q}{p}) $$

ë“±í˜¸ëŠ” q/pê°€ ìƒìˆ˜ì¼ë•Œ, ì¦‰ q/p=1

Deep Learning: KL-divergenceì˜ ê°’ì„ minimize í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ training (ì›ë˜ ë¶„í¬ì™€ ê°€ê¹ê²Œ?)
${ p }$ë¥¼ ${ \hat{p} }$ìœ¼ë¡œ ê·¼ì‚¬ (empirical distribution)

ìµœí›„ì¶”ì •ë²•(ì‚¬ì§„ ì°¸ê³ ) MLE

distanceê°€ ì•„ë‹ˆë¼ divergenceë¼ê³  ë¶€ë¥´ëŠ” ì´ìœ 
pì™€ qì— ëŒ€ì¹­ì„±ì´ ì—†ì–´ì„œ.
ìƒí™©ì— ë”°ë¼ì„œ që¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ëŠ” ê²½ìš°ë„ ìˆìŒ ë” í¸ë¦¬í•œ ìª½ì„ ì„ íƒ.

ìœ„ ì½”ë“œì˜ˆì‹œì—ì„œ KL(p||q) ì™€ KL(q||p) ê³„ì‚°í•´ë³´ê¸° (ë˜‘ê°™ì´ ë‚˜ì˜´)

ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°: ë‹¤ë¥´ê²Œ ë‚˜ì˜¤ëŠ” ì˜ˆì‹œ ë§Œë“¤ì–´ë³´ê¸°

review

| name           | inequality                                                            |
| -------------- | --------------------------------------------------------------------- |
| Markov         | ${ P(Y \ge c) \le \frac{E(Y)}{c} }$                                   |
| Chebyshev      | ${ P(\left\lvert Y-EY \right\rvert \ge k\sigma) \le \frac{1}{k^{2}}}$ |
| Cauchy-Schwarz | ${ E(X^{2}) E(Y^{2}) \ge E(X)^{2}E(Y)^{2} }$                          |

SKì¦ê¶Œ JPëª¨ê±´
ì„ ìˆ˜ë¼ë¦¬ ê±°ë˜í•˜ëŠ”ë° ìœ„í—˜ì€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•´ì•¼ì§€...

ì•ìœ¼ë¡œ 1ë…„ê³¼ ê³¼ê±° 1ë…„ì´ ê°™ë‹¤ê³  ëª¨ë¸ì„ ì„¸ìš´ë‹¤. (ê°™ì„ë¦¬ê°€ ì—†ì§€ë§Œ ì‹¤ìš©ì ì¸ ì´ìœ ë¡œ)

financial bootstraping

ê°•ë‚¨ì•„íŒŒíŠ¸ ê°€ê²© ì„ í˜•íšŒê·€ ì‹¤ì œë¡œ ê³„ì‚°í•´ë³´ê¸°

[Omitted-variable bias - Wikipedia](https://en.wikipedia.org/wiki/Omitted-variable_bias): í˜„ì‹¤ì—ì„œ ë„ˆë¬´ ë„ˆë¬´ ë§ë‹¤

[Bias (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Bias_(statistics))

aggregation bias

regression effect

regressionì˜ ìœ ë˜: ê°ˆíŠ¼ì˜ í‚¤ ì—°êµ¬

$$ \left\lvert b \right\rvert \le \frac{SD_{Y}}{SD_{X}} $$

ìŠ¤í¬ì¸  ê¸°ìë“¤ì´ ë£¨í‚¤ë³´ê³  ìŠ¬ëŸ¼í”„ë¼ê³  ì˜¤íŒ

ê²½ì œí•™ìë“¤ì´ GDP íšŒê·€ë¶„ì„ í•˜ëŠ”ë° ìˆ˜ë ´í• ê±°ë¼ê³  ì˜¤íŒ

íšŒê·€ë¶„ì„ì˜ ê¸°ìš¸ê¸°ê°€ ì‹œê°„ì´ ê°ˆ ìˆ˜ë¡ 0ìœ¼ë¡œ ê°€ëŠ” ì´ìœ ì˜ ì˜¬ë°”ë¥¸ í•´ì„ì€ Yì˜ ì°¨ì´ë¥¼ ë” ì´ìƒ Xì˜ ì°¨ì´ë¡œ ì„¤ëª…í• ê²Œ ì—†ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. Yì˜ ì°¨ì´ê°€ ì—†ë‹¤ê°€ ì•„ë‹˜.

ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°
- [x] binomial of stock price
- [x] KL-divergence ë¹„ëŒ€ì¹­ì¸ê±° ì°¾ì•„ë³´ê¸°
- [ ] í™•ë¥ ë¡  ë¶€ë“±ì‹ ì¦ëª…
- [ ] ë¶€íŠ¸ìŠ¤íŠ¸ë©í•‘ í—·ê°ˆë¦¬ëŠ”ê±° ì˜ ìƒê°í•´ë³´ê¸°
- [x] financial bootstraping ì°¾ì•„ë³´ê¸°
- [x] ì „ë¯¸ë¶„ì´ ì™œ ì „ë¯¸ë¶„ì¸ì§€ ìƒê°í•´ë³´ê¸°
- [ ] ê°•ë‚¨ì•„íŒŒíŠ¸ ê°€ê²© ë°ì´í„° ë°›ì•„ì„œ íŒŒì´ì¬ìœ¼ë¡œ ì„ í˜•íšŒê·€ ì‹¤ì œë¡œ ê³„ì‚°í•´ë³´ê¸°
- [ ] ê°•ë‚¨ì•„íŒŒíŠ¸ ê°€ê²©ì„ í†µí•´ì„œ ì™œ ë³€ìˆ˜ í†µì œê°€ ì¤‘ìš”í•œì§€ ìƒê°í•´ë³´ê¸° (omitted-variable bias) í”„ë¡œì ì…˜ ì´ë¦¬ì €ë¦¬ í•´ë³´ê¸°.
- [ ] ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²• ì˜¤ì°¨ì˜ í•œê³„ ìƒê°í•´ë³´ê¸° (ì˜¤ëŠ˜ë°°ìš´ ë‚´ìš© (ì•„ë§ˆ ì œê³±ê·¼ì˜ ë²•ì¹™?)ê³¼ ì—°ê´€í•´ì„œ ìƒê°í•´ë³´ê¸°

## 2025-07-14

[Confounding - Wikipedia](https://en.wikipedia.org/wiki/Confounding)

accuracy maximize ë³´ë‹¨ objective oriented inference

### ğŸ“ ë¥˜ê·¼ê´€ êµìˆ˜ë‹˜ì˜ ë§¥ë½ì—ì„œ "Objective-Oriented Inference"ë€?

ë‹¤ìŒê³¼ ê°™ì€ ì•„ì´ë””ì–´ì— ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤:

#### âœ… **ëª©ì  ì§€í–¥ ì¶”ë¡  vs. ì§„ë¦¬ ì§€í–¥ ì¶”ë¡ **

- **ì§„ë¦¬ ì§€í–¥ì  ì¶”ë¡  (Truth-oriented inference)**:  
    â†’ "ëª¨ìˆ˜(parameter)ì˜ ì§„ì§œ ê°’ì„ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì¶”ì •í•˜ëŠëƒ"ì— ì´ˆì   
    â†’ ì „í†µì ì¸ í†µê³„í•™(ì˜ˆ: Neyman-Pearson, classical estimation)
    
- **ëª©ì  ì§€í–¥ì  ì¶”ë¡  (Objective-oriented inference)**:  
    â†’ "ê·¸ ì¶”ì •ì´ ì‹¤ì œ ì˜ì‚¬ê²°ì •ì´ë‚˜ ì •ì±… ëª©í‘œì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠëƒ"ì— ì´ˆì   
    â†’ ì¶”ì •ì˜ **ì •í™•ì„±**ë³´ë‹¤ **ìœ ìš©ì„±/íš¨ìš©ì„±**ì´ ì¤‘ìš”  
    â†’ ì˜ˆ: ì˜ëª»ëœ ì¶”ì •ì´ë”ë¼ë„, ì •ì±…ê²°ì •ì— ë„ì›€ì´ ëœë‹¤ë©´ ê·¸ê²Œ ë” ì¤‘ìš”í•œ ê²ƒ
    

---

### ğŸ“Œ ì˜ˆì‹œ (ê²½ì œí•™Â·ì •ì±… ë¶„ì„ ê´€ì ì—ì„œ)

- ì–´ë–¤ êµìœ¡ì •ì±…ì´ í•™ìƒì˜ ì„±ì ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë¶„ì„í•  ë•Œ:
    
    - **ì§„ë¦¬ ì§€í–¥ì  ì¶”ë¡ **: ì •ì±…ì˜ íš¨ê³¼ë¥¼ ì—„ë°€íˆ ì¶”ì •í•˜ëŠ” ë° ì´ˆì .
        
    - **ëª©ì  ì§€í–¥ì  ì¶”ë¡ **: ì •ì±…ì´ ì‹¤ì œë¡œ ì„±ê³¼ë¥¼ ê°œì„ í•˜ê²Œ ìœ ë„í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ê²ƒì— ì´ˆì .
        
- ì˜ë£Œë¹„ ë°ì´í„°ë¥¼ ë¶„ì„í•  ë•Œ:
    
    - ì§„ë¦¬ ì§€í–¥: í‰ê·  ì˜ë£Œë¹„ ì •í™•íˆ ì¶”ì •.
        
    - ëª©ì  ì§€í–¥: ê·¸ ì¶”ì •ê°’ì´ ë³´í—˜ë£Œ ì±…ì •ì´ë‚˜ ì •ì±… ì„¤ê³„ì— ì˜ ì‘ë™í•˜ëŠëƒê°€ ë” ì¤‘ìš”.
        

---

### ğŸ“˜ ë¥˜ êµìˆ˜ë‹˜ì˜ êµì¬ë‚˜ ê°•ì˜ ìŠ¬ë¼ì´ë“œì—ì„  ì£¼ë¡œ ë‹¤ìŒì²˜ëŸ¼ ìš”ì•½ë©ë‹ˆë‹¤:

> â€œìš°ë¦¬ëŠ” ë‹¨ì§€ parameter ê°’ì„ ì •í™•íˆ ì¶”ì •í•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼,  
> ê·¸ ì¶”ì •ì´ ì˜ì‚¬ê²°ì •ì— ì–´ë–¤ ë„ì›€ì„ ì£¼ëŠ”ì§€ì— ê´€ì‹¬ì´ ìˆë‹¤.â€

ì´ ê´€ì ì€ **í†µê³„í•™ì´ ì‹¤ì²œì  ì˜ì‚¬ê²°ì • ë„êµ¬ë¡œì„œ ê¸°ëŠ¥í•´ì•¼ í•œë‹¤**ëŠ” ì‹¤ìš©ì£¼ì˜ì  ì² í•™ì— ê¸°ë°˜í•œ ê²ƒì´ê³ , íŠ¹íˆ **ì •ì±…í•™, ê³„ëŸ‰ê²½ì œí•™, ì‚¬íšŒê³¼í•™** ìª½ì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ê²Œ ë‹¤ë¤„ì§‘ë‹ˆë‹¤.

---

### ğŸ” ìš”ì•½

|êµ¬ë¶„|ì„¤ëª…|
|---|---|
|**Truth-oriented**|parameter ê°’ì„ ì •í™•íˆ ì¶”ì •í•˜ë ¤ëŠ” ì „í†µì  í†µê³„ ì ‘ê·¼|
|**Objective-oriented**|ì‹¤ìš©ì Â·ì •ì±…ì  ëª©ì ì— ë§ëŠ” ìœ ìš©í•œ ì¶”ë¡ ì„ ê°•ì¡°|

[Hedonic regression - Wikipedia](https://en.wikipedia.org/wiki/Hedonic_regression)

ë°ì´í„°ë¥¼ ë¬´ì‘ì • ë§ì´ ë„£ëŠ”ë‹¤ê³  ì¢‹ì€ê²Œ ì•„ë‹ˆë‹¤, í†µì œëœ ë°ì´í„°ë¥¼ ë„£ì§€ ì•Šìœ¼ë©´ í†µê³„ë¥¼ ì™œê³¡í•˜ê²Œ ëœë‹¤.

[ë…¸ë²¨ìƒì´ ë°íŒ ë‚¨ë…€ ì„ê¸ˆê²©ì°¨ì˜ ì§„ì‹¤](https://brunch.co.kr/@brazilclub/132)

linear spline regressionë„ ì¤‘íšŒê·€ ë¶„ì„:
ê³„ìˆ˜ ${ \beta_{1},\beta_{2} }$ ë‘ ê°œë¡œ 2ì°¨ì› í‰ë©´ì„ ë§Œë“¤ê¸° ë•Œë¬¸ì¸ê°€?

ë„ë©”ì¸ ì§€ì‹ì´ ì—†ëŠ” ìƒíƒœë¡œ ë°ì´í„° ë¶„ì„ì„ í•˜ëŠ” ê²ƒë§Œí¼ ìœ„í—˜í•œê²Œ ì—†ë‹¤.

ì¡°ì •ëœ ê²°ì •ê³„ìˆ˜: ììœ ë„ë¡œ ë‚˜ëˆ ì¤˜ì„œ ì„¤ëª…ë³€ìˆ˜ê°€ ì •ë§ë¡œ ì œëŒ€ë¡œ ì—­í• ì„ í•˜ëŠ”ì§€ íŒë‹¨í•  ìˆ˜ ìˆë‹¤. <- ì§€ê¸ˆì€ ì˜ ì•ˆ ì”€

KNN: í•œ ì‚¬ëŒì„ ì•Œë ¤ë©´, ê·¸ ì‚¬ëŒì˜ ì¹œêµ¬ë¥¼ ë³´ë¼.
birds of a feather flock together, ìœ ìœ ìƒì¢…

${ y = f(x) + \varepsilon }$

$$ y - \hat{f}(x) = \overbrace{y - E(y \mid x)}^{A} + \overbrace{E(y\mid x) - f(x)}^{B} + \overbrace{f(x) - \hat{f}(x)}^{C} $$

Want ${ E[y\mid x] }$

- A: ì—ë¼ ëª¨ë¥´ê² ë‹¤
- B: estimation error
- C: sampling error


$$ \text{MSE} := E\left(\left( f(x) - \hat{f}(x)\right) \right) = \sigma^{2} + \mathrm{Bias}^{2} + \mathrm{Var}^{2} $$

- ${ \sigma^{2} = E(A^{2}) }$
- ${ \mathrm{Bias}^{2} = E(B^{2}) }$
- ${ \mathrm{Var}^{2} = E(C^{2}) }$

- [ ] A,B,C ìŒë§ˆë‹¤ correlationì´ 0ì¸ê±° ìƒê°í•´ë³´ê¸°

íŒ¨ë„í‹°ë¥¼ ì£¼ëŠ” ì„¸ê¸°ì— ë”°ë¼ ëª¨ë¸ì´ ê²°ì •ëœë‹¤. 

[Hyperparameter (Bayesian statistics) - Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_(Bayesian_statistics))

[Additive smoothing - Wikipedia](https://en.wikipedia.org/wiki/Additive_smoothing)

[Perronâ€“Frobenius theorem - Wikipedia](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem)

[Shrinkage (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Shrinkage_(statistics))

[out-of-sample error](https://en.wikipedia.org/wiki/Generalization_error)

## 2025-07-21

$$ y_{i} = x_{i}' \beta + \varepsilon_{i} $$

$$ \underbracket{y}_{n \times 1} = \underbracket{X}_{n \times k} \underbracket{\beta}_{k \times 1} + \underbracket{\varepsilon}_{n \times 1} $$

[Ridge regression - Wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)

[Lasso (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Lasso_\(statistics\)): ëš±ëš±í•œ ë°ì´í„°ì—ì„œ ë§ì´ ì“´ë‹¤(=ì¼€ì´ìŠ¤ì— ë¹„í•´ ë°ì´í„°ê°€ ë§ì€ê²ƒ). ë§ì§€ ì•Šì€ ì¼€ì´ìŠ¤ ê°€ì§€ê³  ëŒ€í‘œë¥¼ ë½‘ëŠ” ê²½ìš° ìœ ìš©í•˜ë‹¤.

> accuracy-simplicity tradeoff ê°€ì§€ê³  regulationì„ ì§ê´€í™”í•  ìˆ˜ ìˆë‚˜? - ë‚˜

í†µê³„í•™ì€ ê¸°ê³„ì ìœ¼ë¡œ ìˆ˜ì‹ì— ëŒ€ì…í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë‹¤ë£° ê²ƒì¸ì§€ ì´í•´í•´ì•¼ í•œë‹¤.

testì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ testëŠ” ë³„ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šê³  ìƒì‚°ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒ. ë¹…ë°ì´í„° ì‹œëŒ€ì—ëŠ” reject ì•ˆë˜ë©´ ê·¸ê²Œ ê¸°ì ì´ë‹¤.

runs test: í•˜ë‚˜ì˜ runì€ ê°™ì€ ë¶€í˜¸ì˜ ì—°ì†
runì´ ë§ë‹¤: mean reversion
positive correlation = momentumì´ ìˆìœ¼ë©´ runì´ ì¤„ì–´ë“ ë‹¤.

ê·€ë¬´ê°€ì„¤: momentumì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
ëŒ€ë¦½ê°€ì„¤: momentumì´ ì¡´ì¬í•œë‹¤.

+6, -4ë¡œ ì´ë£¨ì–´ì§„ ë¬´

ê°€ì„¤ê²€ì •: ì¢‹ë‹¤ ë‹ˆë§ì´ ë§ë‹¤ ì¹˜ì

[Model selection - Wikipedia](https://en.wikipedia.org/wiki/Model_selection)

[Regularization (mathematics) - Wikipedia](https://en.wikipedia.org/wiki/Regularization_\(mathematics\))

[Z-test - Wikipedia](https://en.wikipedia.org/wiki/Z-test)

[Student's t-test - Wikipedia](https://en.wikipedia.org/wiki/Student%27s_t-test)

[p-value - Wikipedia](https://en.wikipedia.org/wiki/P-value)

[Sign test - Wikipedia](https://en.wikipedia.org/wiki/Sign_test)

[Chi-squared test - Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test): ì£¼ì‚¬ìœ„ê°€ ê· í˜•ì¡í˜”ëƒ?

[í†µê³„ë°ì´í„°ì„¼í„°](https://data.kostat.go.kr/sbchome/index.do): ì‚¬ì—…ì„ í•  ë•Œ ì´ìš©í•´ë³´ë¼

ëª¨ë“  ìœ ê¶Œìë³´ë‹¨ íˆ¬í‘œí•  ê²ƒ ê°™ì€ ìœ ê¶Œìê°€ ë” ì í•©í•˜ë‹¤. screening questionìœ¼ë¡œ ì–´ëŠì •ë„ ì•Œì•„ë‚¼ ìˆ˜ ìˆìŒ. í‘œë³¸ì´ ë‹¨ìˆœíˆ ë§ë‹¤ê³  ì¢‹ì€ ê²ƒì´ ì•„ë‹ˆë‹¤.

ì‘ë‹µí¸ì˜: ì¡°ì‚¬í•˜ëŠ” ì‚¬ëŒì„ ì‹¤ë§ì‹œí‚¤ì§€ ì•Šìœ¼ë ¤ê³  ì†ë§ˆìŒì„ ê°ì¶œ ìˆ˜ë„ ìˆë‹¤.

[Dummy variable (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Dummy_variable_\(statistics\))

$$ y_{it} = \alpha + \gamma d_{i}+ \varepsilon_{it} $$

$$ i= \begin{dcases}
1 & \text{if } i \in \mathrm{BG} \\
0 & \text{otherwise}
\end{dcases} $$

$$ \gamma = \gamma_{0} + \gamma_{1}d_{t1} + \cdots + \gamma_{m}d_{tm} $$

where $$ d_{tj} = \begin{dcases}
1 & \text{if } t=j \\
0 & \text{otherwise}
\end{dcases} $$
ì‹œê¸°ë³„ë¡œ ë¶„ì„í•˜ë©´ ê¸°ì¡´í•™ìë“¤ì´ ë¶„ì„í•œê²ƒê³¼ ì •ë°˜ëŒ€

[Error function - Wikipedia](https://en.wikipedia.org/wiki/Error_function)

ì™œ í‘œì¤€í¸ì°¨ê°€ ì•„ë‹ˆë¼ í‘œì¤€í‘œì°¨ì¸ê°€? ì–´ë–¤ í‘œì¤€ì˜¤ì°¨ë¥¼ ì“°ëŠ”ì§€ê°€ ë” ì¢‹ì€ ì§ˆë¬¸ì´ë‹¤.

ë‹µ: ë‹¹ì‹ ì´ ì“°ëŠ” í†µê³„ëŸ‰ì˜ í‘œì¤€ì˜¤ì°¨.

- [ ] runs test í‘œ ë§Œë“¤ì–´ë³´ê¸°
- [ ] t-ê°’ê³¼ ë¹…ë°ì´í„°ì˜ ê´€ê³„ë¥¼ ìƒê°í•´ë³´ê¸°

ì¡°ë³„ í”„ë¡œì íŠ¸ëŠ” accuracyê°€ ì•„ë‹ˆë¼ [Sharpe ratio](https://en.wikipedia.org/wiki/Sharpe_ratio) ê·¹ëŒ€í™”

8ì›” 7ì¼ ì¡°ë³„ í”„ë¡œì íŠ¸ ë°œí‘œ. ë°œí‘œì‹œê°„ 20ë¶„ ì§€ë‚˜ë©´ ì¹¼ ê°™ì´ ìë¥¼ ê²ƒì´ë‹¤. ìˆ˜ì‹ ë“¤ì´ë°€ê³  ìˆ˜ì‹ë§Œ ì„¤ëª…í•˜ëŠ” ê²ƒë³´ë‹¨ ì˜ë¯¸ ìœ„ì£¼ë¡œ ì„¤ëª….

[Implied volatility - Wikipedia](https://en.wikipedia.org/wiki/Implied_volatility)

## 2025-07-23

shaply valueê°€ ì¸ê³¼ê´€ê³„ë¥¼ ë§í•´ì£¼ì§„ ì•ŠìŒ. ì¤‘êµ­ ì§€ì—­ë³„ ì„±ì¥ì†ë„ ì°¨ì´ëŠ” ì™¸êµ­ìë³¸ ë•ë¶„ì¸ê°€? ì• ì´ˆì— ì™¸êµ­ ìë³¸ì€ ì„±ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³³ì— ëª¨ì¸ë‹¤.

í˜„ì‹¤ì—ì„œëŠ” ê°€ì„¤ ê²€ì •ì€ ë³„ë¡œ ìœ ìš©í•˜ì§€ ì•Šê³  out-of-sampleì„ ë§ì€ ë°˜ë³µì„ í†µí•´ constantë¥¼ ì •í•œë‹¤. roburstness check.

ê°€ì¥ ì¢‹ì€ ëŠ¥ë ¥ì€ ì¢‹ì€ ë¬¸ì œë¥¼ ì¶œì œí•˜ëŠ” ëŠ¥ë ¥ (ìŠ˜í˜í„°: í˜ì‹ )

[Joseph Schumpeter - Wikipedia](https://en.wikipedia.org/wiki/Joseph_Schumpeter)

[k-anonymity - Wikipedia](https://en.wikipedia.org/wiki/K-anonymity)

[k-nearest neighbors algorithm - Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)

- [ ] kê°€ ë³€í•  ë•Œ ë¬´ì—‡ì´ ì–´ë–»ê²Œ trade-off ë˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° 

supervised learning: regression, indifference curve ë“±ë“±

[Self-supervised learning - Wikipedia](https://en.wikipedia.org/wiki/Self-supervised_learning): next word prediction

[Reinforcement learning - Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)

[Hallucination (artificial intelligence) - Wikipedia](https://en.wikipedia.org/wiki/Hallucination_\(artificial_intelligence\)) = dynamic forecast?

[k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)

[Dendrogram - Wikipedia](https://en.wikipedia.org/wiki/Dendrogram)

í´ëŸ¬ìŠ¤í„° ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ì¬ì•¼ í•œë‹¤.

Q. ë¬´ì—‡ì´ ì¢‹ì€ ê±°ë¦¬ëƒ? (ë‘ ì§‘ë‹¨ì„ í•©ì¹  ë•Œ ì˜ˆì‹œ)
1. í‰ê· ì ê³¼ í‰ê· ì  ì‚¬ì´ì˜ ê±°ë¦¬ (ë‘ ì§‘ë‹¨ì˜ ì‚¬ëŒë“¤ì´ í‰ê· ì ìœ¼ë¡œ ì˜ ì–´ìš¸ë¦¬ëŠ”ì§€?)
2. ë‘ í´ëŸ¬ìŠ¤í„° ì  ì‚¬ì´ ê±°ë¦¬ë“¤ì˜ í‰ê· 
3. ê±°ë¦¬ë“¤ì˜ ìµœì†Œê°’ (í•œëª…ì´ë¼ë„ ê°€ê¹Œìš´ ì‚¬ëŒì´ ìˆìœ¼ë©´ ê·¸ ì‚¬ëŒ í†µí•´ì„œ ë‹¤ ì¹œí•´ì§)
4. ê±°ë¦¬ë“¤ì˜ ìµœëŒ€ê°’ (í•œëª…ì´ë¼ë„ ì‚¬ì´ê°€ ì•ˆ ì¢‹ìœ¼ë©´ ë¶„ìœ„ê¸° ì°ë )

ë¹…ë°ì´í„°ê°€ ì •ë³´ê²©ì°¨ë¥¼ ì—†ì• ì„œ ê²½ì œí•™ì ìœ¼ë¡œ ë†€ë¼ìš´ ì¼ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤.
: ì¹´ì…°ì–´ë§, ì—ì–´ë¹„ì•¤ë¹„, ë ˆëª¬ ë§ˆì¼“ ë“± í”Œë«í¼ì—ì„œ ìœ„í—˜ì„ í•„í„°ë§í•´ì„œ ê°€ëŠ¥.

deep learningì€ ì¤‘ê°„ì— ë°˜ë“œì‹œ ë¹„ì„ í˜•ë³€í™˜ì„ í•´ì•¼ëœë‹¤ ì•ˆí•˜ë©´ ì„ í˜•ë³€í™˜ í•˜ë‚˜ë¡œ ë¬´ë„ˆì§

[Backpropagation - Wikipedia](https://en.wikipedia.org/wiki/Backpropagation)

[Isoquant curve](https://en.wikipedia.org/wiki/Isoquant) v.s. [Indifference curve](https://en.wikipedia.org/wiki/Indifference_curve)

## 2025-07-28

Today

Tree
- c.f. variance decomposition formula: within v.s. between
- bootstrapping ${ \implies }$ "a frest than a tree"
- sampling o featuress ${ \implies }$ decorrelation among trees
XIA and Shapley
information theory

[Law of total variance - Wikipedia](https://en.wikipedia.org/wiki/Law_of_total_variance)

$$ \operatorname{Var}(Y) = \operatorname{E}\operatorname{Var}(Y \mid X) + \operatorname{Var}(\operatorname{E}(Y \mid X)) $$

total var = within, undexplained var + between, explaind var

íŠ¹ì • ë°ì´í„°ê°€ bootstrapingì— í¬í•¨ë˜ì§€ ì•Šì„ í™•ë¥ 

n = sample  size

$$ (1-\frac{1}{n})^{n} \rightarrow e^{-1} = 0.367879 \dots $$

Suppose ${ Y_{1},Y_{2} \sim (\mu,\sigma^{2}) }$

$$ \operatorname{Var}(\overline{Y}) = \frac{\sigma^{2}+\sigma^{2}+2\rho\sigma}{4}=\frac{\sigma^{2}}{2}(1+\rho) $$

where ${ \overline{Y} = \frac{Y_{1}+Y_{2}}{2} }$ and ${ \rho = \operatorname{Corr(Y_{1}, Y_{2}}) }$

boostingì€ replicationì´ ì•„ë‹ˆë‹¤ (ì•™ìƒë¸”ê³¼ ê´€ë ¨ ì—†ë‹¤) <- ë§ì´ë“¤ í˜¼ë™í•´ì„œ ì“´ë‹¤.

Madisonì´ ì“´ê±°ëƒ Hamilton

[Naive Bayes classifier - Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier): ì‹¤ì œë¡œëŠ” ë…ë¦½ì´ ì•„ë‹ˆì§€ë§Œ ì‹¤ìš©ì ìœ¼ë¡œ ë…ë¦½ì´ë¼ ê°€ì •(naiveí•œ ê°€ì •)í•˜ê³  ë² ì´ì¦ˆ ë²•ì¹™ì„ ì ìš©

$$ \frac{P(M_{1}\mid T_{1},\dots,T_{m})}{P(M_{2}\mid T_{1},\dots,T_{m})} = \frac{P(M_{1})}{P(M_{2})} \frac{P(T_{1}, \dots,T_{m} \mid M_{1})}{P(T_{1},\dots,T_{m} \mid M_{2})}$$

posterior odds ratio = prior odds ratio ${ \times }$ likelihood ratio

$$ y = f(x_{1},\dots,x_{k}) + \varepsilon $$

1. prediction
2. explaining the predictive outcom, XAI, Shapley
3. causal inference

cooperative gameì—ì„œ ìœ ì¼í•œ value

[Marginal value - Wikipedia](https://en.wikipedia.org/wiki/Marginal_value)

ì„¸ ë‹¹ì˜ ì˜ì„ì´ 40% 40% 11%ì¸ë° ì§€ë¶„ì€ 1/3 1/3 1/3ìœ¼ë¡œ ë™ì¼

[Heat map - Wikipedia](https://en.wikipedia.org/wiki/Heat_map)

shapely valueëŠ” ê³µì¥ì„ ëª¨ë‹ˆí„°ë§ í•˜ëŠ” ê²ƒì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

## 2025-07-29

${ p }$: "empirical" real probability
${ q }$: "counter-factural" probability, hypothetical

$$ H = E_{p}\log_{2}(\frac{1}{p}),\ \mathrm{CE} = E_{p}\log_{2}(\frac{1}{q}) $$

${ \mathrm{KL} = \mathrm{CE} - H }$

$$ \mathrm{KL} = - E_{p} \log_{2}(\frac{q}{p}) \ge - \log_{2}E_{p}(\frac{q}{p}) =0$$

ë“±í˜¸ëŠ” ${ p = q }$ ë§Œ ì„±ë¦½ ${ \implies }$KL divegenceë¥¼ ìµœì†Œí™” í•˜ëŠ” ê²ƒì€ ì§„ì‹¤ì— ê°€ê¹Œì´ ê°€ëŠ” ê²ƒ.

$$ \min_{q} \mathrm{CE} \iff \min_{q} \mathrm{KL} $$

$$ \begin{eqnarray}
\min_{q} \mathrm{CE} & = & \min_{q} -E_{p}\log_{2} q \\
& \approx & \min_{q} -\hat{E}\log_{2}q \\
& = & \min_{q} -\frac{1}{n} \sum_{i=1}^{n} \log_{2} q_{i} \\
& \iff & \underbracket{\max_{q} \log_{2}L}_{\text{ìµœìš° ì¶”ì •ë²•}}
\end{eqnarray} $$

[Maximum likelihood estimation - Wikipedia](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)

${ Y_{-} }$: own history of ${ Y }$
${ X_{-} }$: cross history

$$ H(Y \mid Y_{-}) \ge H(Y \mid Y_{-},X_{-}) $$

TE: transfer entropy

$$ \mathrm{TE} = H(Y \mid Y_{-}) -H(Y \mid Y_{-},X_{-})  $$

[Transfer entropy - Wikipedia](https://en.wikipedia.org/wiki/Transfer_entropy)

ì£¼ì‹ì‹œì¥ì— ëŒ€ì…í•˜ì—¬ ì´í•´í•´ë³´ê¸°.

[Autoregressive model - Wikipedia](https://en.wikipedia.org/wiki/Autoregressive_model)

[Granger causality - Wikipedia](https://en.wikipedia.org/wiki/Granger_causality): transfer entropyì˜ ì¼ì¢…

[Cholesky decomposition - Wikipedia](https://en.wikipedia.org/wiki/Cholesky_decomposition)

[[ML] Precision ê³¼ Recallì˜ Trade-off, ê·¸ë¦¬ê³  ROC Curve](https://techblog-history-younghunjo1.tistory.com/101)